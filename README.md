# AWRQ: Activation-aware Weight Reformulation Quantization for Large Language Models


# Quick start



# Main Results
- Results of LLaMA and LLaMA-2 families on zero-shot tasks at W4A8 (4-bit per-channel weight, 8-bit per-tensor activation) quantization.
![image](https://github.com/zl200881/AWRQ/assets/17473403/2b264eaf-a6d4-458b-9f8e-c49ad2a8595b)

- Results of OPT families via perplexity evaluation on WikiText2 at W4A8 quantization.
![image](https://github.com/zl200881/AWRQ/assets/17473403/46287f96-f9f6-4741-9630-8ae9449370c4)

# Acknowledgements
[GPTQ](https://github.com/IST-DASLab/gptq)
[SmoothQuant](https://github.com/mit-han-lab/smoothquant)
