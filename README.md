# AWRQ: Activation-aware Weight Reformulation Quantization for Large Language Models


# Quick start



# Main Results
- Results of LLaMA and LLaMA-2 families on zero-shot tasks at W4A8 (4-bit per-channel weight, 8-bit per-tensor activation) quantization

| Model | Method | PIQA | ARC-e | ARC-c | BoolQ | COPA | StoryCloze | Avg. $\uparrow$ |
| ----- | ------ | ------ | ------ | ------ | ------ | ------ | ------ |   
|LLaMA-1-7B | FP16        | 78.35 | 67.30 | 38.23 | 73.15 | 84.00 | 76.07 | 69.52 |
|                             | RTN | 70.02 | 56.61 | 33.79 | 61.56 | 83.00 | 72.37 | 62.89 |
|                             | SmoothQuant | 70.35 | 58.12 | 32.68 | 61.90 | 86.00 | 72.95 | 63.67 |
|                             | AWRQ        | 73.61 | 59.22 | 32.00 | 71.41 | 82.00 | 73.20 | {\bf 65.24} |
| ----- | ------ | ------ | ------ | ------ | ------ | ------ | ------ |
|LLaMA-1-13B | FP16        | 78.94 | 74.54 | 43.94 | 68.53 | 90.00 | 78.17 | 72.35 |
|                             | RTN | 72.58 | 66.84 | 36.77 | 67.00 | 88.00 | 75.05 | 67.71 |
|                             | SmoothQuant | 74.32 | 64.14 | 35.41 | 63.21 | 86.00 | 73.90 | 66.16 |
|                             | AWRQ        | 75.68 | 68.69 | 38.57 | 69.05 | 88.00 | 74.16 | {\bf 69.03} |
| ----- | ------ | ------ | ------ | ------ | ------ | ------ | ------ |
|LLaMA-1-30B | FP16        | 80.96 | 75.34 | 46.76 | 68.38 | 90.00 | 78.87 | 73.39 |
|                             | RTN | 71.11 | 65.70 | 39.16 | 66.21 | 81.00 | 70.46 | 65.61 |
|                             | SmoothQuant | 76.22 | 71.42 | 42.75 | 67.98 | 90.00 | 73.97 | 70.39 |
|                             | AWRQ        | 77.15 | 72.90 | 43.60 | 79.08 | 96.00 | 77.85 | {\bf 74.43} |
| ----- | ------ | ------ | ------ | ------ | ------ | ------ | ------ |
|LLaMA-2-7B | FP16        | 78.24 | 69.32 | 39.33 | 71.28 | 88.00 | 77.59 | 70.63 |
|                             | RTN | 71.06 | 61.07 | 33.28 | 59.60 | 79.00 | 71.93 | 62.66 |
|                             | SmoothQuant | 71.87 | 63.80 | 35.15 | 62.08 | 83.00 | 71.61 | 64.59 |
|                             | AWRQ        | 75.57 | 66.29 | 37.46 | 68.17 | 83.00 | 76.07 | {\bf 67.76} |
| ----- | ------ | ------ | ------ | ------ | ------ | ------ | ------ |
| LLaMA-2-13B | FP16        | 78.73 | 72.98 | 45.65 | 69.17 | 92.00 | 78.74 | 72.88 |
|                             | RTN |  73.78 | 67.59 | 38.74 | 66.45 | 80.00 | 73.52 | 66.68 |
|                             | SmoothQuant | 74.86 | 66.75 | 35.67 | 63.70 | 84.00 | 73.77 | 66.46 |
|                             | AWRQ        | 75.03 | 67.34 | 37.37 | 66.30 | 87.00 | 75.56 | {\bf 68.10} |
  

- 

# Acknowledgements
[GPTQ](https://github.com/IST-DASLab/gptq)
[SmoothQuant](https://github.com/mit-han-lab/smoothquant)
